{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-17T09:08:25.534961Z",
     "iopub.status.busy": "2025-09-17T09:08:25.534279Z",
     "iopub.status.idle": "2025-09-17T09:08:25.538779Z",
     "shell.execute_reply": "2025-09-17T09:08:25.538125Z",
     "shell.execute_reply.started": "2025-09-17T09:08:25.534923Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from tqdm import tqdm\n",
    "from dataset import CustomImageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T09:00:43.168002Z",
     "iopub.status.busy": "2025-09-17T09:00:43.167640Z",
     "iopub.status.idle": "2025-09-17T09:00:53.624192Z",
     "shell.execute_reply": "2025-09-17T09:00:53.623583Z",
     "shell.execute_reply.started": "2025-09-17T09:00:43.167976Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Vechur': 140, 'Mehsana': 95, 'Hallikar': 186, 'Amritmahal': 94, 'Kankrej': 179, 'Sahiwal': 439, 'Surti': 64, 'Jersey': 203, 'Pulikulam': 125, 'Nagpuri': 187, 'Nagori': 89, 'Malnad_gidda': 107, 'Dangi': 82, 'Murrah': 173, 'Jaffrabadi': 102, 'Red_Dane': 167, 'Krishna_Valley': 136, 'Guernsey': 119, 'Kherigarh': 36, 'Rathi': 149, 'Khillari': 113, 'Bargur': 94, 'Banni': 109, 'Holstein_Friesian': 328, 'Toda': 124, 'Alambadi': 99, 'Deoni': 99, 'Kangayam': 91, 'Kenkatha': 55, 'Kasargod': 95, 'Nimari': 84, 'Tharparkar': 217, 'Bhadawari': 86, 'Ongole': 191, 'Red_Sindhi': 166, 'Hariana': 130, 'Umblachery': 76, 'Gir': 372, 'Ayrshire': 234, 'Brown_Swiss': 225, 'Nili_Ravi': 89}\n"
     ]
    }
   ],
   "source": [
    "root_folder = \"Indian_bovine_breeds\"  # change this to your dataset path\n",
    "\n",
    "class_counts = {}\n",
    "for class_folder in os.listdir(root_folder):\n",
    "    class_path = os.path.join(root_folder, class_folder)\n",
    "    if os.path.isdir(class_path):\n",
    "        num_files = len([\n",
    "            f for f in os.listdir(class_path)\n",
    "            if os.path.isfile(os.path.join(class_path, f))\n",
    "        ])\n",
    "        class_counts[class_folder] = num_files\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T09:00:53.625080Z",
     "iopub.status.busy": "2025-09-17T09:00:53.624836Z",
     "iopub.status.idle": "2025-09-17T09:00:53.631402Z",
     "shell.execute_reply": "2025-09-17T09:00:53.630903Z",
     "shell.execute_reply.started": "2025-09-17T09:00:53.625062Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Sahiwal', 439],\n",
       " ['Gir', 372],\n",
       " ['Holstein_Friesian', 328],\n",
       " ['Ayrshire', 234],\n",
       " ['Brown_Swiss', 225],\n",
       " ['Tharparkar', 217],\n",
       " ['Jersey', 203],\n",
       " ['Ongole', 191],\n",
       " ['Nagpuri', 187],\n",
       " ['Hallikar', 186],\n",
       " ['Kankrej', 179],\n",
       " ['Murrah', 173],\n",
       " ['Red_Dane', 167],\n",
       " ['Red_Sindhi', 166],\n",
       " ['Rathi', 149],\n",
       " ['Vechur', 140],\n",
       " ['Krishna_Valley', 136],\n",
       " ['Hariana', 130],\n",
       " ['Pulikulam', 125],\n",
       " ['Toda', 124],\n",
       " ['Guernsey', 119],\n",
       " ['Khillari', 113],\n",
       " ['Banni', 109],\n",
       " ['Malnad_gidda', 107],\n",
       " ['Jaffrabadi', 102],\n",
       " ['Alambadi', 99],\n",
       " ['Deoni', 99],\n",
       " ['Mehsana', 95],\n",
       " ['Kasargod', 95],\n",
       " ['Amritmahal', 94],\n",
       " ['Bargur', 94],\n",
       " ['Kangayam', 91],\n",
       " ['Nagori', 89],\n",
       " ['Nili_Ravi', 89],\n",
       " ['Bhadawari', 86],\n",
       " ['Nimari', 84],\n",
       " ['Dangi', 82],\n",
       " ['Umblachery', 76],\n",
       " ['Surti', 64],\n",
       " ['Kenkatha', 55],\n",
       " ['Kherigarh', 36]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_class_counts = [[k,v] for k,v in class_counts.items()]\n",
    "sorted_class_counts = sorted(sorted_class_counts, key=lambda x: x[1], reverse=True)\n",
    "sorted_class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T09:00:53.632332Z",
     "iopub.status.busy": "2025-09-17T09:00:53.632110Z",
     "iopub.status.idle": "2025-09-17T09:00:53.652630Z",
     "shell.execute_reply": "2025-09-17T09:00:53.652089Z",
     "shell.execute_reply.started": "2025-09-17T09:00:53.632307Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def split_dataset_paths(root_folder, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, seed=1337):\n",
    "    np.random.seed(seed)\n",
    "    dataset_splits = {}\n",
    "\n",
    "    for cls in sorted(os.listdir(root_folder)):\n",
    "        cls_path = os.path.join(root_folder, cls)\n",
    "        if not os.path.isdir(cls_path):\n",
    "            continue\n",
    "        images = [os.path.join(cls_path, f) for f in os.listdir(cls_path) if os.path.isfile(os.path.join(cls_path, f))]\n",
    "        np.random.shuffle(images)\n",
    "\n",
    "        n_total = len(images)\n",
    "        # n_train = int(n_total * train_ratio)\n",
    "        n_val = max(int(n_total * val_ratio), 5)\n",
    "        n_test = max(int(n_total * test_ratio), 5)\n",
    "        n_train = n_total - n_val - n_test\n",
    "\n",
    "        train_files = images[:n_train]\n",
    "        val_files = images[n_train:n_train + n_val]\n",
    "        test_files = images[n_train + n_val:]\n",
    "\n",
    "        dataset_splits[cls] = {\n",
    "            'train': train_files,\n",
    "            'val': val_files,\n",
    "            'test': test_files\n",
    "        }\n",
    "\n",
    "    return dataset_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T09:00:53.655035Z",
     "iopub.status.busy": "2025-09-17T09:00:53.654511Z",
     "iopub.status.idle": "2025-09-17T09:00:56.037842Z",
     "shell.execute_reply": "2025-09-17T09:00:56.037110Z",
     "shell.execute_reply.started": "2025-09-17T09:00:53.655017Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = split_dataset_paths(\"Indian_bovine_breeds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T09:00:56.038842Z",
     "iopub.status.busy": "2025-09-17T09:00:56.038639Z",
     "iopub.status.idle": "2025-09-17T09:00:56.044310Z",
     "shell.execute_reply": "2025-09-17T09:00:56.043614Z",
     "shell.execute_reply.started": "2025-09-17T09:00:56.038825Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 5, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['Kherigarh']['train']),len(dataset['Kherigarh']['val']),len(dataset['Kherigarh']['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T09:00:56.045452Z",
     "iopub.status.busy": "2025-09-17T09:00:56.045120Z",
     "iopub.status.idle": "2025-09-17T09:00:56.084491Z",
     "shell.execute_reply": "2025-09-17T09:00:56.083921Z",
     "shell.execute_reply.started": "2025-09-17T09:00:56.045424Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(353, 43, 43)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['Sahiwal']['train']),len(dataset['Sahiwal']['val']),len(dataset['Sahiwal']['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T09:00:56.101048Z",
     "iopub.status.busy": "2025-09-17T09:00:56.100780Z",
     "iopub.status.idle": "2025-09-17T09:00:56.116706Z",
     "shell.execute_reply": "2025-09-17T09:00:56.116031Z",
     "shell.execute_reply.started": "2025-09-17T09:00:56.101027Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_file_list(splits_dict, split_name):\n",
    "    file_list = []\n",
    "    for cls, splits in splits_dict.items():\n",
    "        label = list(splits_dict.keys()).index(cls)  # index of class\n",
    "        paths = splits[split_name]\n",
    "        file_list.extend([(p, label) for p in paths])\n",
    "    return file_list\n",
    "\n",
    "# Define torchvision transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "class BalancedBatchSampler(Sampler):\n",
    "    def __init__(self, labels, samples_per_class=5):\n",
    "        \"\"\"\n",
    "        labels: list or array of class labels aligned with dataset indices\n",
    "        samples_per_class: number of images per class per batch\n",
    "        \"\"\"\n",
    "        self.labels = np.array(labels)\n",
    "        self.samples_per_class = samples_per_class\n",
    "        \n",
    "        # Group indices by class\n",
    "        self.class_indices = defaultdict(list)\n",
    "        for idx, label in enumerate(labels):\n",
    "            self.class_indices[label].append(idx)\n",
    "        \n",
    "        # Calculate number of batches\n",
    "        self.batches_per_epoch = min(len(idxs) // samples_per_class for idxs in self.class_indices.values())\n",
    "        self.classes = list(self.class_indices.keys())\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch = []\n",
    "        # Shuffle indices within each class\n",
    "        shuffled_indices = {}\n",
    "        for c in self.classes:\n",
    "            idxs = self.class_indices[c]\n",
    "            np.random.shuffle(idxs)\n",
    "            shuffled_indices[c] = idxs\n",
    "        \n",
    "        for batch_idx in range(self.batches_per_epoch):\n",
    "            batch.clear()\n",
    "            for c in self.classes:\n",
    "                start = batch_idx * self.samples_per_class\n",
    "                end = start + self.samples_per_class\n",
    "                batch.extend(shuffled_indices[c][start:end])\n",
    "            np.random.shuffle(batch)  # Optional: shuffle final batch indices\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.batches_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T09:00:56.117702Z",
     "iopub.status.busy": "2025-09-17T09:00:56.117430Z",
     "iopub.status.idle": "2025-09-17T09:00:56.138611Z",
     "shell.execute_reply": "2025-09-17T09:00:56.138028Z",
     "shell.execute_reply.started": "2025-09-17T09:00:56.117685Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_files = create_file_list(dataset, 'train')\n",
    "val_files = create_file_list(dataset, 'val')\n",
    "test_files = create_file_list(dataset, 'test')\n",
    "\n",
    "train_dataset = CustomImageDataset(train_files, transform=transform)\n",
    "val_dataset = CustomImageDataset(val_files, transform=transform)\n",
    "test_dataset = CustomImageDataset(test_files, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=True, \n",
    "    prefetch_factor=2,\n",
    "    pin_memory=True,\n",
    "    num_workers=2\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=32, \n",
    "    prefetch_factor=2,\n",
    "    pin_memory=True,\n",
    "    num_workers=2\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=32, \n",
    "    prefetch_factor=2,\n",
    "    pin_memory=True,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T09:00:56.139558Z",
     "iopub.status.busy": "2025-09-17T09:00:56.139331Z",
     "iopub.status.idle": "2025-09-17T09:00:56.160913Z",
     "shell.execute_reply": "2025-09-17T09:00:56.160203Z",
     "shell.execute_reply.started": "2025-09-17T09:00:56.139536Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_files = create_file_list(dataset, 'train')  # as before\n",
    "train_labels = [label for _, label in train_files]\n",
    "\n",
    "train_dataset = CustomImageDataset(train_files, transform=transform)\n",
    "train_sampler = BalancedBatchSampler(train_labels, samples_per_class=1)\n",
    "train_loader = DataLoader(train_dataset, batch_sampler=train_sampler)\n",
    "\n",
    "val_files = create_file_list(dataset, 'val')  # as before\n",
    "val_labels = [label for _, label in val_files]\n",
    "\n",
    "val_dataset = CustomImageDataset(val_files, transform=transform)\n",
    "val_sampler = BalancedBatchSampler(val_labels, samples_per_class=3)\n",
    "val_loader = DataLoader(val_dataset, batch_sampler=val_sampler)\n",
    "\n",
    "test_files = create_file_list(dataset, 'test')  # as before\n",
    "test_labels = [label for _, label in test_files]\n",
    "\n",
    "test_dataset = CustomImageDataset(test_files, transform=transform)\n",
    "test_sampler = BalancedBatchSampler(test_labels, samples_per_class=3)\n",
    "test_loader = DataLoader(test_dataset, batch_sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T09:00:56.161997Z",
     "iopub.status.busy": "2025-09-17T09:00:56.161511Z",
     "iopub.status.idle": "2025-09-17T09:00:56.176850Z",
     "shell.execute_reply": "2025-09-17T09:00:56.176353Z",
     "shell.execute_reply.started": "2025-09-17T09:00:56.161979Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 1, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(val_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T09:00:56.177636Z",
     "iopub.status.busy": "2025-09-17T09:00:56.177465Z",
     "iopub.status.idle": "2025-09-17T09:00:56.194524Z",
     "shell.execute_reply": "2025-09-17T09:00:56.193913Z",
     "shell.execute_reply.started": "2025-09-17T09:00:56.177622Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            # nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            # nn.Conv2d(64, 64, kernel_size=3, padding=1),            \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            # nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            # nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 28 * 28, 256),  # assuming input size 224x224\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T09:00:56.196658Z",
     "iopub.status.busy": "2025-09-17T09:00:56.196437Z",
     "iopub.status.idle": "2025-09-17T09:00:56.214776Z",
     "shell.execute_reply": "2025-09-17T09:00:56.214106Z",
     "shell.execute_reply.started": "2025-09-17T09:00:56.196635Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def train(model, train_loader, val_loader, epochs=10, lr=0.001, device='cuda'):\n",
    "#     model = model.to(device)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         running_loss = 0.0\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "\n",
    "#         for images, labels in tqdm(train_loader):\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             running_loss += loss.item() * images.size(0)\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "\n",
    "#         train_loss = running_loss / total\n",
    "#         train_acc = correct / total\n",
    "\n",
    "#         # Validation\n",
    "#         model.eval()\n",
    "#         val_loss = 0.0\n",
    "#         val_correct = 0\n",
    "#         val_total = 0\n",
    "#         with torch.no_grad():\n",
    "#             for images, labels in tqdm(val_loader):\n",
    "#                 images, labels = images.to(device), labels.to(device)\n",
    "#                 outputs = model(images)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "#                 val_loss += loss.item() * images.size(0)\n",
    "#                 _, predicted = torch.max(outputs, 1)\n",
    "#                 val_total += labels.size(0)\n",
    "#                 val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "#         val_loss /= val_total\n",
    "#         val_acc = val_correct / val_total\n",
    "\n",
    "#         print(f\"Epoch [{epoch + 1}/{epochs}] Train Loss: {train_loss:.4f} Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T09:00:56.215763Z",
     "iopub.status.busy": "2025-09-17T09:00:56.215542Z",
     "iopub.status.idle": "2025-09-17T09:05:43.683863Z",
     "shell.execute_reply": "2025-09-17T09:05:43.683192Z",
     "shell.execute_reply.started": "2025-09-17T09:00:56.215744Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# num_classes = 41  # number of classes\n",
    "# model = SimpleCNN(num_classes)\n",
    "# train(model, train_loader, val_loader, epochs=10, lr=0.001, device='cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T09:08:33.139699Z",
     "iopub.status.busy": "2025-09-17T09:08:33.139425Z",
     "iopub.status.idle": "2025-09-17T09:08:33.155540Z",
     "shell.execute_reply": "2025-09-17T09:08:33.154708Z",
     "shell.execute_reply.started": "2025-09-17T09:08:33.139679Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Episodic Sampler for N-way K-shot + Q query setup\n",
    "class EpisodicBatchSampler(Sampler):\n",
    "    def __init__(self, labels, n_way, k_shot, q_query, episodes_per_epoch):\n",
    "        self.labels = np.array(labels)\n",
    "        self.n_way = n_way\n",
    "        self.k_shot = k_shot\n",
    "        self.q_query = q_query\n",
    "        self.episodes_per_epoch = episodes_per_epoch\n",
    "        self.class_to_indices = defaultdict(list)\n",
    "        for idx, label in enumerate(labels):\n",
    "            self.class_to_indices[label].append(idx)\n",
    "        self.classes = [c for c in self.class_to_indices if len(self.class_to_indices[c]) >= (k_shot + q_query)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.episodes_per_epoch\n",
    "\n",
    "    def __iter__(self):\n",
    "        for _ in range(self.episodes_per_epoch):\n",
    "            batch_indices = []\n",
    "            selected_classes = np.random.choice(self.classes, self.n_way, replace=False)\n",
    "            for cls in selected_classes:\n",
    "                indices = np.random.choice(self.class_to_indices[cls], self.k_shot + self.q_query, replace=False)\n",
    "                batch_indices.extend(indices)\n",
    "            yield batch_indices\n",
    "\n",
    "# Simple CNN embedding model\n",
    "class ProtoNet(nn.Module):\n",
    "    def __init__(self, embedding_dim=64):\n",
    "        super(ProtoNet, self).__init__()\n",
    "\n",
    "        def conv_block(in_channels, out_channels):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(2)  # halve spatial resolution\n",
    "            )\n",
    "\n",
    "        # 224 → 112 → 56 → 28 → 14 → 7\n",
    "        self.encoder = nn.Sequential(\n",
    "            conv_block(3, 8),      # 224 → 112\n",
    "            conv_block(8, 16),    # 112 → 56\n",
    "            conv_block(16, 32),   # 56 → 28\n",
    "            conv_block(32, 64),   # 28 → 14\n",
    "            conv_block(64, embedding_dim),  # 14 → 7\n",
    "            nn.AdaptiveAvgPool2d(1) # always → (1,1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)              # (B, embedding_dim, 1, 1)\n",
    "        return x.view(x.size(0), -1)     # flatten → (B, embedding_dim)\n",
    "\n",
    "        \n",
    "# Prototypical loss function\n",
    "def prototypical_loss(embeddings, targets, n_way, k_shot, q_query):\n",
    "    \"\"\"\n",
    "    embeddings: (batch_size, embedding_dim) for one episode\n",
    "    targets: (batch_size) class labels, mapped 0 to n_way-1 for sampled classes\n",
    "    \"\"\"\n",
    "    support_indices = []\n",
    "    query_indices = []\n",
    "    for i in range(n_way):\n",
    "        support_indices.extend(range(i*(k_shot+q_query), i*(k_shot+q_query) + k_shot))\n",
    "        query_indices.extend(range(i*(k_shot+q_query) + k_shot, (i+1)*(k_shot+q_query)))\n",
    "    support_embeddings = embeddings[support_indices]\n",
    "    query_embeddings = embeddings[query_indices]\n",
    "\n",
    "    # compute prototypes\n",
    "    prototypes = support_embeddings.view(n_way, k_shot, -1).mean(dim=1)  # (n_way, embedding_dim)\n",
    "\n",
    "    # dist between queries and prototypes (batch matrix)\n",
    "    dists = torch.cdist(query_embeddings, prototypes)  # (n_way*q_query, n_way)\n",
    "\n",
    "    # Compute log-probabilities\n",
    "    log_p_y = F.log_softmax(-dists, dim=1)  # negative distances as logits\n",
    "\n",
    "    # Construct query labels\n",
    "    query_labels = torch.arange(n_way).unsqueeze(1).repeat(1, q_query).view(-1).to(embeddings.device)\n",
    "\n",
    "    # Loss is negative log-likelihood of true classes\n",
    "    loss = F.nll_loss(log_p_y, query_labels)\n",
    "\n",
    "    # Accuracy\n",
    "    _, y_hat = log_p_y.max(1)\n",
    "    acc = torch.eq(y_hat, query_labels).float().mean()\n",
    "\n",
    "    return loss, acc\n",
    "\n",
    "# Transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),  # typical size for prototypical nets\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Assume splits_dict is defined as before\n",
    "train_files = create_file_list(dataset, 'train')  # (image_path, label)\n",
    "train_labels = [label for _, label in train_files]\n",
    "\n",
    "# Parameters for prototypical training\n",
    "n_way = 5\n",
    "k_shot = 20\n",
    "q_query = 2\n",
    "episodes_per_epoch = 100\n",
    "\n",
    "train_dataset = CustomImageDataset(train_files, transform=transform)\n",
    "train_sampler = EpisodicBatchSampler(train_labels, n_way, k_shot, q_query, episodes_per_epoch)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_sampler=train_sampler,\n",
    "    num_workers=4,   # use all available cores\n",
    "    prefetch_factor=2,            # workers prefetch batches\n",
    "    persistent_workers=True       # keep workers alive\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-17T09:12:07.774Z",
     "iopub.execute_input": "2025-09-17T09:08:33.214782Z",
     "iopub.status.busy": "2025-09-17T09:08:33.214250Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "# elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "#     device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "model = ProtoNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(10):\n",
    "#     model.train()\n",
    "#     total_loss, total_acc = 0.0, 0.0\n",
    "#     for batch in tqdm(train_loader):\n",
    "#         images, labels = batch\n",
    "#         images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         embeddings = model(images)\n",
    "\n",
    "#         # Remap labels to 0..n_way-1 for the sampled classes within the episode\n",
    "#         # This is necessary because batch includes only n_way classes but labels have global indices\n",
    "#         unique_labels = torch.unique(labels)\n",
    "#         label_map = {l.item(): i for i, l in enumerate(unique_labels)}\n",
    "#         mapped_labels = torch.tensor([label_map[l.item()] for l in labels]).to(device)\n",
    "\n",
    "#         loss, acc = prototypical_loss(embeddings, mapped_labels, n_way, k_shot, q_query)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "#         total_acc += acc.item()\n",
    "\n",
    "#     print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader):.4f}, Acc: {total_acc / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Before training loop\n",
    "# class_prototypes = torch.zeros(41, 64, device=device)\n",
    "# class_counts = torch.zeros(41, device=device)\n",
    "\n",
    "# for epoch in range(20):\n",
    "#     model.train()\n",
    "#     total_loss, total_acc = 0.0, 0.0\n",
    "#     class_correct = defaultdict(int)\n",
    "#     class_total = defaultdict(int)\n",
    "\n",
    "#     for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "#         images, labels = batch\n",
    "#         images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         embeddings = model(images)  # (B, embedding_dim)\n",
    "\n",
    "#         # --- Compute distances to ALL prototypes ---\n",
    "#         dists = torch.cdist(embeddings, class_prototypes)  # (B, num_classes)\n",
    "#         logits = -dists\n",
    "\n",
    "#         # --- Loss & optimization ---\n",
    "#         loss = F.cross_entropy(logits, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "#         preds = logits.argmax(dim=1)\n",
    "#         total_acc += (preds == labels).float().mean().item()\n",
    "\n",
    "#         # --- Per-class accuracy tracking ---\n",
    "#         for t, p in zip(labels, preds):\n",
    "#             class_total[t.item()] += 1\n",
    "#             if t == p:\n",
    "#                 class_correct[t.item()] += 1\n",
    "\n",
    "#         # --- Update prototypes safely ---\n",
    "#         with torch.no_grad():\n",
    "#             for c in labels.unique():\n",
    "#                 emb = embeddings[labels == c].mean(dim=0).detach()\n",
    "#                 count = (labels == c).sum()\n",
    "#                 class_prototypes[c] = (class_prototypes[c] * class_counts[c] + emb * count) / (class_counts[c] + count)\n",
    "#                 class_counts[c] += count\n",
    "\n",
    "#     # --- End of epoch summary ---\n",
    "#     avg_loss = total_loss / len(train_loader)\n",
    "#     avg_acc = total_acc / len(train_loader)\n",
    "#     print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "#     print(f\"  Loss: {avg_loss:.4f}, Accuracy: {avg_acc:.4f}\")\n",
    "\n",
    "#     print(\"  Per-class accuracy:\")\n",
    "#     for cls in range(41):\n",
    "#         if class_total[cls] > 0:\n",
    "#             acc_cls = class_correct[cls] / class_total[cls]\n",
    "#             print(f\"    Class {cls:02d}: {acc_cls:.4f} ({class_correct[cls]}/{class_total[cls]})\")\n",
    "#         else:\n",
    "#             print(f\"    Class {cls:02d}: N/A (not sampled this epoch)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/100 [00:00<?, ?it/s]/Users/fakiraaqilparvez/Documents/DQ 2.0/.venv/lib/python3.12/site-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 1:   4%|▍         | 4/100 [00:05<01:37,  1.01s/it]/Users/fakiraaqilparvez/Documents/DQ 2.0/.venv/lib/python3.12/site-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 1:  27%|██▋       | 27/100 [00:15<00:29,  2.48it/s]/Users/fakiraaqilparvez/Documents/DQ 2.0/.venv/lib/python3.12/site-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 1:  77%|███████▋  | 77/100 [00:35<00:09,  2.54it/s]/Users/fakiraaqilparvez/Documents/DQ 2.0/.venv/lib/python3.12/site-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 1: 100%|██████████| 100/100 [00:43<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary:\n",
      "  Loss: 3.5277, Accuracy: 0.0883\n",
      "  Per-class accuracy:\n",
      "    Class 00: 0.0000 (0/300)\n",
      "    Class 01: 0.0000 (0/264)\n",
      "    Class 02: 0.0461 (13/282)\n",
      "    Class 03: 0.0867 (26/300)\n",
      "    Class 04: 0.0111 (3/270)\n",
      "    Class 05: 0.0062 (2/324)\n",
      "    Class 06: 0.1361 (49/360)\n",
      "    Class 07: 0.3276 (114/348)\n",
      "    Class 08: 0.0000 (0/324)\n",
      "    Class 09: 0.0000 (0/252)\n",
      "    Class 10: 0.0833 (24/288)\n",
      "    Class 11: 0.0000 (0/282)\n",
      "    Class 12: 0.2660 (83/312)\n",
      "    Class 13: 0.2092 (59/282)\n",
      "    Class 14: 0.1667 (56/336)\n",
      "    Class 15: 0.0000 (0/300)\n",
      "    Class 16: 0.0000 (0/306)\n",
      "    Class 17: 0.0000 (0/318)\n",
      "    Class 18: 0.0061 (2/330)\n",
      "    Class 19: 0.6369 (214/336)\n",
      "    Class 20: 0.1727 (57/330)\n",
      "    Class 21: 0.0000 (0/318)\n",
      "    Class 22: 0.0000 (0/288)\n",
      "    Class 23: 0.0621 (22/354)\n",
      "    Class 24: 0.0148 (4/270)\n",
      "    Class 25: 0.1091 (36/330)\n",
      "    Class 26: 0.0595 (15/252)\n",
      "    Class 27: 0.0000 (0/366)\n",
      "    Class 28: 0.0000 (0/288)\n",
      "    Class 29: 0.0068 (2/294)\n",
      "    Class 30: 0.2654 (86/324)\n",
      "    Class 31: 0.0163 (5/306)\n",
      "    Class 32: 0.0000 (0/342)\n",
      "    Class 33: 0.0000 (0/276)\n",
      "    Class 34: 0.5430 (202/372)\n",
      "    Class 35: 0.0109 (3/276)\n",
      "    Class 36: 0.0213 (6/282)\n",
      "    Class 37: 0.0256 (8/312)\n",
      "    Class 38: 0.0686 (21/306)\n",
      "    Class 39: 0.0000 (0/270)\n",
      "    Class 40: 0.0000 (0/330)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 100/100 [00:41<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Summary:\n",
      "  Loss: 3.2174, Accuracy: 0.1774\n",
      "  Per-class accuracy:\n",
      "    Class 00: 0.0387 (13/336)\n",
      "    Class 01: 0.0648 (21/324)\n",
      "    Class 02: 0.2667 (80/300)\n",
      "    Class 03: 0.2901 (94/324)\n",
      "    Class 04: 0.2400 (72/300)\n",
      "    Class 05: 0.0152 (4/264)\n",
      "    Class 06: 0.2133 (64/300)\n",
      "    Class 07: 0.5614 (192/342)\n",
      "    Class 08: 0.0063 (2/318)\n",
      "    Class 09: 0.0128 (4/312)\n",
      "    Class 10: 0.1950 (55/282)\n",
      "    Class 11: 0.0727 (24/330)\n",
      "    Class 12: 0.1932 (51/264)\n",
      "    Class 13: 0.5452 (193/354)\n",
      "    Class 14: 0.1512 (49/324)\n",
      "    Class 15: 0.0000 (0/312)\n",
      "    Class 16: 0.0636 (21/330)\n",
      "    Class 17: 0.0096 (3/312)\n",
      "    Class 18: 0.0316 (11/348)\n",
      "    Class 19: 0.4479 (129/288)\n",
      "    Class 20: 0.6701 (197/294)\n",
      "    Class 21: 0.0035 (1/288)\n",
      "    Class 22: 0.0200 (6/300)\n",
      "    Class 23: 0.1967 (59/300)\n",
      "    Class 24: 0.3529 (108/306)\n",
      "    Class 25: 0.1277 (36/282)\n",
      "    Class 26: 0.1954 (68/348)\n",
      "    Class 27: 0.0000 (0/288)\n",
      "    Class 28: 0.0654 (20/306)\n",
      "    Class 29: 0.1700 (51/300)\n",
      "    Class 30: 0.4874 (155/318)\n",
      "    Class 31: 0.2377 (77/324)\n",
      "    Class 32: 0.0425 (13/306)\n",
      "    Class 33: 0.0000 (0/294)\n",
      "    Class 34: 0.4697 (124/264)\n",
      "    Class 35: 0.1733 (52/300)\n",
      "    Class 36: 0.1667 (57/342)\n",
      "    Class 37: 0.0870 (24/276)\n",
      "    Class 38: 0.2881 (102/354)\n",
      "    Class 39: 0.0100 (3/300)\n",
      "    Class 40: 0.0000 (0/246)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 100/100 [00:40<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Summary:\n",
      "  Loss: 3.0067, Accuracy: 0.2518\n",
      "  Per-class accuracy:\n",
      "    Class 00: 0.1293 (38/294)\n",
      "    Class 01: 0.3148 (102/324)\n",
      "    Class 02: 0.4138 (144/348)\n",
      "    Class 03: 0.2788 (87/312)\n",
      "    Class 04: 0.6242 (191/306)\n",
      "    Class 05: 0.0544 (16/294)\n",
      "    Class 06: 0.1961 (60/306)\n",
      "    Class 07: 0.5667 (170/300)\n",
      "    Class 08: 0.0278 (8/288)\n",
      "    Class 09: 0.0000 (0/300)\n",
      "    Class 10: 0.2540 (64/252)\n",
      "    Class 11: 0.1451 (47/324)\n",
      "    Class 12: 0.3459 (110/318)\n",
      "    Class 13: 0.5986 (176/294)\n",
      "    Class 14: 0.2138 (59/276)\n",
      "    Class 15: 0.0111 (3/270)\n",
      "    Class 16: 0.1250 (30/240)\n",
      "    Class 17: 0.0494 (16/324)\n",
      "    Class 18: 0.0586 (19/324)\n",
      "    Class 19: 0.4236 (122/288)\n",
      "    Class 20: 0.7787 (271/348)\n",
      "    Class 21: 0.0145 (4/276)\n",
      "    Class 22: 0.0632 (22/348)\n",
      "    Class 23: 0.2853 (89/312)\n",
      "    Class 24: 0.3050 (86/282)\n",
      "    Class 25: 0.4248 (130/306)\n",
      "    Class 26: 0.1803 (53/294)\n",
      "    Class 27: 0.0163 (5/306)\n",
      "    Class 28: 0.2186 (80/366)\n",
      "    Class 29: 0.4375 (147/336)\n",
      "    Class 30: 0.4746 (131/276)\n",
      "    Class 31: 0.2467 (74/300)\n",
      "    Class 32: 0.1122 (35/312)\n",
      "    Class 33: 0.0000 (0/294)\n",
      "    Class 34: 0.3056 (99/324)\n",
      "    Class 35: 0.4113 (153/372)\n",
      "    Class 36: 0.3440 (97/282)\n",
      "    Class 37: 0.0714 (18/252)\n",
      "    Class 38: 0.3548 (132/372)\n",
      "    Class 39: 0.2138 (68/318)\n",
      "    Class 40: 0.0497 (17/342)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 100/100 [00:40<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Summary:\n",
      "  Loss: 2.8501, Accuracy: 0.3017\n",
      "  Per-class accuracy:\n",
      "    Class 00: 0.1489 (42/282)\n",
      "    Class 01: 0.3105 (95/306)\n",
      "    Class 02: 0.4048 (119/294)\n",
      "    Class 03: 0.2848 (94/330)\n",
      "    Class 04: 0.6789 (167/246)\n",
      "    Class 05: 0.1321 (42/318)\n",
      "    Class 06: 0.2424 (80/330)\n",
      "    Class 07: 0.6384 (203/318)\n",
      "    Class 08: 0.0426 (12/282)\n",
      "    Class 09: 0.0248 (7/282)\n",
      "    Class 10: 0.2364 (61/258)\n",
      "    Class 11: 0.2530 (85/336)\n",
      "    Class 12: 0.3571 (105/294)\n",
      "    Class 13: 0.5667 (153/270)\n",
      "    Class 14: 0.2340 (66/282)\n",
      "    Class 15: 0.0233 (6/258)\n",
      "    Class 16: 0.2327 (74/318)\n",
      "    Class 17: 0.1019 (33/324)\n",
      "    Class 18: 0.0852 (23/270)\n",
      "    Class 19: 0.3129 (92/294)\n",
      "    Class 20: 0.9167 (341/372)\n",
      "    Class 21: 0.0621 (19/306)\n",
      "    Class 22: 0.0723 (23/318)\n",
      "    Class 23: 0.3500 (105/300)\n",
      "    Class 24: 0.4599 (149/324)\n",
      "    Class 25: 0.5345 (186/348)\n",
      "    Class 26: 0.3278 (118/360)\n",
      "    Class 27: 0.0243 (7/288)\n",
      "    Class 28: 0.3366 (103/306)\n",
      "    Class 29: 0.5247 (170/324)\n",
      "    Class 30: 0.4748 (151/318)\n",
      "    Class 31: 0.4020 (123/306)\n",
      "    Class 32: 0.1813 (62/342)\n",
      "    Class 33: 0.0093 (3/324)\n",
      "    Class 34: 0.1512 (39/258)\n",
      "    Class 35: 0.5430 (202/372)\n",
      "    Class 36: 0.3229 (93/288)\n",
      "    Class 37: 0.1914 (62/324)\n",
      "    Class 38: 0.3167 (95/300)\n",
      "    Class 39: 0.4673 (143/306)\n",
      "    Class 40: 0.1512 (49/324)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 100/100 [00:40<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Summary:\n",
      "  Loss: 2.7207, Accuracy: 0.3509\n",
      "  Per-class accuracy:\n",
      "    Class 00: 0.2619 (77/294)\n",
      "    Class 01: 0.4712 (147/312)\n",
      "    Class 02: 0.4333 (117/270)\n",
      "    Class 03: 0.3482 (117/336)\n",
      "    Class 04: 0.7305 (206/282)\n",
      "    Class 05: 0.2340 (73/312)\n",
      "    Class 06: 0.2345 (83/354)\n",
      "    Class 07: 0.7049 (203/288)\n",
      "    Class 08: 0.1190 (35/294)\n",
      "    Class 09: 0.0340 (11/324)\n",
      "    Class 10: 0.3878 (114/294)\n",
      "    Class 11: 0.3302 (105/318)\n",
      "    Class 12: 0.4259 (138/324)\n",
      "    Class 13: 0.6122 (180/294)\n",
      "    Class 14: 0.3814 (119/312)\n",
      "    Class 15: 0.0747 (26/348)\n",
      "    Class 16: 0.3523 (93/264)\n",
      "    Class 17: 0.1964 (66/336)\n",
      "    Class 18: 0.1273 (42/330)\n",
      "    Class 19: 0.4253 (148/348)\n",
      "    Class 20: 0.9113 (257/282)\n",
      "    Class 21: 0.0850 (25/294)\n",
      "    Class 22: 0.0988 (32/324)\n",
      "    Class 23: 0.5278 (171/324)\n",
      "    Class 24: 0.5804 (195/336)\n",
      "    Class 25: 0.3962 (126/318)\n",
      "    Class 26: 0.3227 (91/282)\n",
      "    Class 27: 0.0341 (9/264)\n",
      "    Class 28: 0.4434 (141/318)\n",
      "    Class 29: 0.6069 (193/318)\n",
      "    Class 30: 0.5467 (164/300)\n",
      "    Class 31: 0.4277 (136/318)\n",
      "    Class 32: 0.2455 (81/330)\n",
      "    Class 33: 0.0271 (7/258)\n",
      "    Class 34: 0.4475 (145/324)\n",
      "    Class 35: 0.2482 (70/282)\n",
      "    Class 36: 0.3741 (110/294)\n",
      "    Class 37: 0.0526 (12/228)\n",
      "    Class 38: 0.3679 (117/318)\n",
      "    Class 39: 0.5863 (197/336)\n",
      "    Class 40: 0.1321 (42/318)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6:  53%|█████▎    | 53/100 [00:23<00:20,  2.24it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# --- Loss & optimization ---\u001b[39;00m\n\u001b[32m     38\u001b[39m loss = F.cross_entropy(logits, labels)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m optimizer.step()\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# --- Batch accuracy ---\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DQ 2.0/.venv/lib/python3.12/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DQ 2.0/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DQ 2.0/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- Define model with learnable prototypes ---\n",
    "class ProtoClassifier(nn.Module):\n",
    "    def __init__(self, backbone, num_classes=41, embedding_dim=64):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        # prototypes are learnable parameters\n",
    "        self.class_prototypes = nn.Parameter(\n",
    "            torch.randn(num_classes, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.backbone(x)  # (B, embedding_dim)\n",
    "        dists = torch.cdist(embeddings, self.class_prototypes)  # (B, num_classes)\n",
    "        logits = -dists\n",
    "        return logits, embeddings\n",
    "\n",
    "\n",
    "# --- Setup ---\n",
    "num_classes = 41\n",
    "embedding_dim = 64\n",
    "model = ProtoClassifier(ProtoNet(embedding_dim), num_classes, embedding_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# --- Training loop ---\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    total_loss, total_acc = 0.0, 0.0\n",
    "    class_correct = defaultdict(int)\n",
    "    class_total = defaultdict(int)\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits, embeddings = model(images)\n",
    "\n",
    "        # --- Loss & optimization ---\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # --- Batch accuracy ---\n",
    "        preds = logits.argmax(dim=1)\n",
    "        total_loss += loss.item()\n",
    "        total_acc += (preds == labels).float().mean().item()\n",
    "\n",
    "        # --- Per-class accuracy tracking ---\n",
    "        for t, p in zip(labels, preds):\n",
    "            class_total[t.item()] += 1\n",
    "            if t == p:\n",
    "                class_correct[t.item()] += 1\n",
    "\n",
    "    # --- End of epoch summary ---\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_acc = total_acc / len(train_loader)\n",
    "    print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "    print(f\"  Loss: {avg_loss:.4f}, Accuracy: {avg_acc:.4f}\")\n",
    "\n",
    "    print(\"  Per-class accuracy:\")\n",
    "    for cls in range(num_classes):\n",
    "        if class_total[cls] > 0:\n",
    "            acc_cls = class_correct[cls] / class_total[cls]\n",
    "            print(f\"    Class {cls:02d}: {acc_cls:.4f} \"\n",
    "                  f\"({class_correct[cls]}/{class_total[cls]})\")\n",
    "        else:\n",
    "            print(f\"    Class {cls:02d}: N/A (not sampled this epoch)\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 3103253,
     "sourceId": 5344733,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
